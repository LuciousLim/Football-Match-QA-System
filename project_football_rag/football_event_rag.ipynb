{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59da627d-6ea8-4d2d-8574-b654e999c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:27\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "done\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813860 sha256=94933dfd31595098d4c1cf91235cf0faefc4651c0d5586d9266e0c05f8098f56\n",
      "  Stored in directory: /Users/zhangman/Library/Caches/pip/wheels/00/e3/92/8594f4cee2c9fd4ad82fe85e4bf2559ab8ea84ef19b1dd3d15\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]m1/2\u001b[0m [pyspark]\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2947d-793a-4a3c-854c-1bd2cc15af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before run, install pyspark first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed86406-647c-40f9-9939-2c174953887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Spark to clean/transform/join large datasets, then embed in parallel (careful with model size).\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "# this is read from hadoop, but can use read from csv first to test\n",
    "spark = spark.read.csv(\"hdfs://namenode:8020/path/to/file.csv\")\n",
    "spark.range(5).show()\n",
    "csv_opts = {\n",
    "    \"header\": True,\n",
    "    \"inferSchema\": True,\n",
    "    \"mode\": \"DROPMALFORMED\",\n",
    "    \"nullValue\": \"NA\"  # treat \"NA\" as null\n",
    "}\n",
    "\n",
    "df1 = spark.read.options(**csv_opts).csv(\"/path/to/file1.csv\")\n",
    "df2 = spark.read.options(**csv_opts).csv(\"/path/to/file2.csv\")\n",
    "df3 = spark.read.options(**csv_opts).csv(\"/path/to/file3.csv\")\n",
    "# example join on event id (adjust keys to your real ones)\n",
    "events = df1.unionByName(df2).unionByName(df3)\n",
    "joined = df_events.join(df_matches, \"id_event\", \"left\") \\\n",
    "                  .join(df_players, \"player\", \"left\")\n",
    "\n",
    "#2) Load your code dictionaries (TXT) and map codes → labels\n",
    "\"\"\"\n",
    "0   Announcement\n",
    "1   Attempt\n",
    "2   Corner\n",
    "...\n",
    "\"\"\"\n",
    "dict_schema = StructType([\n",
    "    StructField(\"code\", IntegerType(), True),\n",
    "    StructField(\"label\", StringType(), True)\n",
    "])\n",
    "\n",
    "# If it’s tab-separated:\n",
    "event_type_dict = spark.read.csv(\"/path/to/event_type.txt\", sep=\"\\t\", schema=dict_schema)\n",
    "# If it’s space-separated, use sep=\" \" (or read as text and split).\n",
    "\n",
    "event_type_dict = event_type_dict.withColumnRenamed(\"label\", \"event_type_name\")\n",
    "\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "events_clean = events.select(\n",
    "    \"id_event\", \"time\", \"text\", \"event_type\", \"event_type_name\",\n",
    "    \"event_team\", \"opponent\", \"player\", \"player2\",\n",
    "    \"shot_place\", \"shot_outcome\", \"is_goal\", \"location\",\n",
    "    \"bodypart\", \"assist_method\", \"situation\", \"fast_break\"\n",
    ")\n",
    "\n",
    "# Ensure types / defaults (example)\n",
    "events_clean = events_clean.withColumn(\"is_goal\", when(col(\"is_goal\") == 1, lit(True)).otherwise(lit(False)))\n",
    "\n",
    "doc = concat_ws(\" \",\n",
    "    when(col(\"event_team\").isNotNull(), concat_ws(\"\", lit(\"[Team: \"), col(\"event_team\"), lit(\"]\"))).otherwise(lit(\"\")),\n",
    "    when(col(\"opponent\").isNotNull(), concat_ws(\"\", lit(\"[Opp: \"), col(\"opponent\"), lit(\"]\"))).otherwise(lit(\"\")),\n",
    "    when(col(\"event_type_name\").isNotNull(), concat_ws(\"\", lit(\"[Type: \"), col(\"event_type_name\"), lit(\"]\"))).otherwise(lit(\"\")),\n",
    "    when(col(\"is_goal\")==True, lit(\"[Goal]\")).otherwise(lit(\"\")),\n",
    "    when(col(\"time\").isNotNull(), concat_ws(\"\", lit(\"[T=\"), col(\"time\").cast(\"string\"), lit(\"]\"))).otherwise(lit(\"\")),\n",
    "    col(\"text\")\n",
    ")\n",
    "\"\"\"\n",
    "Build a readable document string + metadata for RAG\n",
    "RAG works best if each row is turned into a self-contained chunk. \n",
    "Compose a doc field that mixes your text with important structured fields.\n",
    "\"\"\"\n",
    "doc = concat_ws(\" \",\n",
    "    when(col(\"event_team\").isNotNull(), concat_ws(\"\", lit(\"[Team: \"), col(\"event_team\"), lit(\"]\"))).otherwise(lit(\"\")),\n",
    "    when(col(\"opponent\").isNotNull(), concat_ws(\"\", lit(\"[Opp: \"), col(\"opponent\"), lit(\"]\"))).otherwise(lit(\"\")),\n",
    "    when(col(\"event_type_name\").isNotNull(), concat_ws(\"\", lit(\"[Type: \"), col(\"event_type_name\"), lit(\"]\"))).otherwise(lit(\"\")),\n",
    "    when(col(\"is_goal\")==True, lit(\"[Goal]\")).otherwise(lit(\"\")),\n",
    "    when(col(\"time\").isNotNull(), concat_ws(\"\", lit(\"[T=\"), col(\"time\").cast(\"string\"), lit(\"]\"))).otherwise(lit(\"\")),\n",
    "    col(\"text\")\n",
    ")\n",
    "# here need to draw graphs to showcase the dataset, also show some generated text\n",
    "events_docs = events_clean.withColumn(\"doc\", doc)\n",
    "rows = events_docs.select(\"id_event\",\"doc\",\"event_team\",\"opponent\",\"event_type_name\").limit(50000).collect()\n",
    "texts = [r[\"doc\"] for r in rows]\n",
    "metas = [{\"id_event\": r[\"id_event\"],\n",
    "          \"event_team\": r[\"event_team\"],\n",
    "          \"opponent\": r[\"opponent\"],\n",
    "          \"event_type\": r[\"event_type_name\"]} for r in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d9a70-8476-46fb-9863-251a7d38b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-community chromadb sentence-transformers tiktoken\n",
    "# Optional: OpenAI client if you’ll call an LLM API\n",
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79f0e29-2cf8-4e53-9aa2-ece1ab34654e",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (199116483.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31m_IncompleteInputError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#RAG\n",
    "\"\"\"\n",
    " RAG (common pattern)\n",
    "\n",
    "Pipeline overview:\n",
    "\t1.\tIngest: Load raw data (PDFs, MD, CSVs, Parquet, DBs).\n",
    "\t2.\tSplit: Chunk text (e.g., 500–1000 tokens with overlap).\n",
    "\t3.\tEmbed: Turn chunks into vectors.\n",
    "\t4.\tIndex: Store in a vector DB (Chroma/FAISS/Pinecone/Weaviate/Qdrant/Milvus).\n",
    "\t5.\tRetrieve: At query time, embed the question → nearest-neighbor search.\n",
    "\t6.\tGenerate: Feed top-k chunks + question into an LLM with a grounding prompt.\n",
    "\t7.\tEvaluate/iterate: Adjust chunk size, rerankers, prompts; add metadata filters.\n",
    "\n",
    "Popular libraries:\n",
    "\t•\tLangChain (chains, retrievers, integrations)\n",
    "\"\"\"\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split into chunks (good for longer doc fields)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
    "texts_split, metas_split = [], []\n",
    "for t, m in zip(texts, metas):\n",
    "    for chunk in splitter.split_text(t):\n",
    "        texts_split.append(chunk)\n",
    "        metas_split.append(m)\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=texts_split,\n",
    "    embedding=emb,\n",
    "    metadatas=metas_split,\n",
    "    collection_name=\"sports-events\"\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "q = \"Show me attempts by Hamburg that were missed.\"\n",
    "docs = retriever.get_relevant_documents(q)\n",
    "for d in docs[:3]:\n",
    "    print(d.page_content, d.metadata)\n",
    "\n",
    "#Using LangChain with RAG and <30B LLMs\n",
    "# install runtime if needed\n",
    "!pip install ollama\n",
    "# run a local model (separate terminal)\n",
    "ollama run mistral\n",
    "# Example with a local model via Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOllama(model=\"mistral\")  # or \"llama3\" etc.\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type=\"stuff\")\n",
    "\n",
    "print(qa.run(\"List events where Hamburg had a left-footed shot that missed.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9f5ca-ffa0-4eff-a58a-ed8f920bd306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
